{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install autoawq\n!pip install auto-round[gpu]\n\n\n!pip install auto-round[cpu]\n!pip install optimum\n!pip install auto-gptq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kill -9 <PID>\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRoundConfig\nimport os\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nbackend = \"auto\"  ##cpu, hpu, cuda\nquantization_config = AutoRoundConfig(\n    backend=backend\n)\nquantized_model_path = \"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit\"\nmodel = AutoModelForCausalLM.from_pretrained(quantized_model_path,\n                                             device_map=backend.split(':')[0],\n                                             quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\ntext = \"There is a girl who likes adventure,\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\nprint(tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"شغال ع المعالج ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRoundConfig\nimport os\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nbackend = \"cpu\"  # Change to CPU\nquantization_config = AutoRoundConfig(\n    backend=backend\n)\nquantized_model_path = \"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit\"\nmodel = AutoModelForCausalLM.from_pretrained(quantized_model_path,\n                                             device_map=backend.split(':')[0],\n                                             quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\ntext = \"There is a girl who likes adventure,\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\nprint(tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRoundConfig\nimport os\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nbackend = \"cuda\"  # Change to CPU\nquantization_config = AutoRoundConfig(\n    backend=backend\n)\nquantized_model_path = \"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit\"\nmodel = AutoModelForCausalLM.from_pretrained(quantized_model_path,\n                                             device_map=backend.split(':')[0],\n                                             quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\ntext = \"There is a girl who likes adventure,\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\nprint(tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRoundConfig\nimport os\nimport torch\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nbackend = \"cuda0 \"  # Change to CPU\nquantization_config = AutoRoundConfig(\n    backend=backend\n)\nquantized_model_path = \"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit\"\nmodel = AutoModelForCausalLM.from_pretrained(quantized_model_path,\n                                             device_map=backend.split(':')[0],\n                                             quantization_config=quantization_config)\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\ntext = \"There is a girl who likes adventure,\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\nprint(tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"شغال ع الجميع جيد","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom auto_round import AutoRoundConfig\nimport os\nimport torch\n\n# تنظيف ذاكرة GPU\ntorch.cuda.empty_cache()\ntorch.cuda.reset_peak_memory_stats()\n\n# ضبط إعدادات تخصيص الذاكرة في PyTorch\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# تحديد إعدادات التحميل\nbackend = \"auto\"  # سيتم توزيع النموذج تلقائيًا بين GPU و CPU\nquantization_config = AutoRoundConfig(\n    backend=backend\n)\n\n# تحديد المسار إلى النموذج\nquantized_model_path = \"kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit\"\n\n# تعيين تخصيص الذاكرة لكل جهاز\nmax_memory = {\n    0: \"13GiB\",  # تخصيص 6 جيجابايت على كودا 0\n    1: \"13GiB\",  # تخصيص 6 جيجابايت على كودا 1\n    \"cpu\": \"30GiB\"  # استخدام 30 جيجابايت على CPU\n}\n\n# تحميل النموذج مع التوزيع على الأجهزة المختلفة\nmodel = AutoModelForCausalLM.from_pretrained(\n    quantized_model_path,\n    device_map=\"auto\",\n    max_memory=max_memory,\n    quantization_config=quantization_config\n)\n\n# تحميل التوكنايزر\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\n\n# إدخال النص\ntext = \"There is a girl who likes adventure,\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n\n# توليد النص\noutput = model.generate(**inputs, max_new_tokens=5)\n\n# فك التشفير وطباعة الناتج\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-12T03:17:29.836624Z","iopub.execute_input":"2025-03-12T03:17:29.836966Z","iopub.status.idle":"2025-03-12T03:19:55.072278Z","shell.execute_reply.started":"2025-03-12T03:17:29.836908Z","shell.execute_reply":"2025-03-12T03:19:55.071442Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/auto_round/auto_quantizer.py:190: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['target_backend']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n  warnings.warn(warning_msg)\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n  def backward(ctx, grad_output):\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @custom_fwd(cast_inputs=torch.float16)\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5055: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n/usr/local/lib/python3.10/dist-packages/accelerate/utils/imports.py:318: UserWarning: Intel Extension for PyTorch 2.6 needs to work with PyTorch 2.6.*, but PyTorch 2.5.1+cu121 is found. Please switch to the matching version and run again.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d870bc11a27e40619126ef6507e1ba60"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"There is a girl who likes adventure, she loves to explore new\n","output_type":"stream"}],"execution_count":1}]}